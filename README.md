# Vandy-CS8395-mini-Challenge
Certainly! Here's the README.md content for your project:

---

### Debugging Mastery with Generative AI

This repository contains an automated assessment designed to measure the debugging abilities of Generative AI models, specifically contrasting the debugging prowess of an AI model against that of OpenAI's ChatGPT.

#### Novelty:

1. **Debugging Mastery**: While many assessments focus on an AI's coding ability, our unique approach evaluates the debugging capability. This skill is crucial for developers and is an innovative way to measure AI's proficiency.
   
2. **Detailed Review**: The assessment doesn't just return a binary "correct/incorrect" verdict. It provides a comprehensive review, comparing the AI's solution to ChatGPT's solution based on various criteria.

3. **Diverse Difficulty Levels**: The problems span different difficulty levels, from beginner to expert, ensuring a thorough evaluation.

#### How to Run the Benchmark:

1. **Setup**:
   - Clone this repository.
   - Ensure you have the `openai` library installed: `pip install openai`.
   - Set up your OpenAI API key by replacing `'YOUR_API_KEY_HERE'` in the code.

2. **Running**:
   - Execute the main script: `python assessment_script.py`.
   - Input the text for evaluation when prompted.

3. **Output**:
   - The script will print a detailed assessment, comparing the debugging abilities of the AI model against ChatGPT's.

#### Criteria for Assessment:

1. **Correctness**: Evaluates if the AI model made the right corrections.
2. **Comparison with ChatGPT**: Examines if the AI model made more or fewer changes than ChatGPT to fix the code.
3. **Efficiency**: Compares the space and time complexity between the AI model's corrected version and ChatGPT's corrected version.

#### Scoring:

The score ranges from 0 to 100, with ChatGPT's debugging ability benchmarked at 80. The AI model's score will be higher or lower based on its relative performance against ChatGPT.

#### Conclusion:

This assessment provides a novel way to evaluate the debugging capabilities of Generative AI models. It offers a detailed review, making it an invaluable tool for understanding and benchmarking AI's code debugging proficiency.




